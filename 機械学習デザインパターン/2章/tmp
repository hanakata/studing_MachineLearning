うまくいく理由
    語彙に含まれない入力
        新しいものが出てきてもハッシュ化された特徴量は[0,9]の範囲になるため対応可能
    値の種類の多さ
        種類が多くてもハッシュ化することでサイズが抑えられるので対応可能
    コールドスタート
        追加されてもとりあえず他の情報に基づく予測を用いて情報が増えた段階で再訓練することで予測反映が可能
トレードオフと代替案
    バケットの衝突
        特徴量ハッシュの実装において剰余の演算は損失を伴う
            特徴量ハッシュは複数のカテゴリ型の入力が同じハッシュバケットを共有してかまわない場合に限る
    分布の歪み
        カテゴリ型の入力の分布が均等でない場合正解率は著しく低下する
    集中的な特徴量
        カテゴリ型変数の分布が歪んでいたりバケット数が少ない場合はモデルの入力として集約的な特徴量を追加する

埋め込み
    問題
        機械学習モデルはモデルの入力特徴量の特製と出力ラベルとの関連をとらえるパターンデータ内で体系的に探索する
            入力特徴量のデータ表現は最終モデルの品質に直接影響する
    解決
        埋め込みパターン
            値の種類数の多いデータを低次元で密に表現するという問題に訓練可能な重みを持つ埋め込みそうに入力データを渡すことで対処
        テキストの埋め込み
            テキストは埋め込み層の利用が有効
        画像の埋め込み
            本質的には入力画像の埋め込みとなる
                畳み込みニューラルネットワーク
    上手くいく理由
        埋め込み層はニューラルネットワークの1つの隠れ層にすぎない
    トレードオフと代替案
        埋め込み次元の選択
            適切な次元数は実務家が選択
                どの程度の次元を選択するか
                    トレードオフを考慮し選択
        オートエンコーダ
            教師ありの場合は埋め込み訓練には多くのラベルが必要となる
                オートエンコーダを利用できる
        文脈言語モデル
            Word2VecやBERTなどのマスクされた言語モデルによりラベル不足問題解消
        データウェアハウス内の埋め込み
            構造化データの機械学習はデータウェアハウスのSQLで直接実行が最適
                問題は構造化データと自然言語テキストなどとの組み合わせ

特徴量クロス
    問題
        二値分類器を作成するタスクを考える
            x1およびx2の座標のみを利用する場合、＋クラスとークラスを分離する線形の境界を見出せない
    解決
        特徴エンジニアリングとは
            機械学習のプロセスを助けモデルの予測能力を高める新しい特徴量を問題領域の知識を用いて作成するプロセス
        特徴量クロス
            2つ以上のカテゴリ型特徴量間の相互作用を捉えるために特徴量を連結して得られる合成特徴量
    上手くいく理由
        シンプルなモデルにより高い複雑さと表現力を与えより多くの内容を表現可能
        大規模なデータにも拡張適用可能
    トレードオフと代替案
        数値特徴量の取り扱い
            連続型の入力データの特徴量クロスで取り得るすべての値の列挙は不可能。
        値の種類の多さの扱い
            得られるカテゴリの値の種類数は入力特徴量の値と種類数に対して乗法的に増加
                モデルへの入力は疎となる
    正則化の必要性
        値の種類の多い2つのカテゴリ型特徴量をクロスさせるとかけ合わせた数のする意を持つクロス特徴量が生成

マルチモーダル
    問題
        画像データとメタデータを取り扱わせたい
        入力の1つが自由形式テキストである構造化データモデルを訓練
    解決
        テキストとメタデータを組み合わせる
    トレードオフと代替案
        表形式データの複数の表現方法
        テキストのマルチモーダル表現
            複数の方法によるテキストデータの表現
        画像のマルチモーダル表現